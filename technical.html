<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Technical</title>
    <link rel="icon" href="img/omr_logo_icononly.png">
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <!-- animate CSS -->
    <link rel="stylesheet" href="css/animate.css">
    <!-- owl carousel CSS -->
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <!-- themify CSS -->
    <link rel="stylesheet" href="css/themify-icons.css">
    <!-- flaticon CSS -->
    <link rel="stylesheet" href="css/flaticon.css">
    <!-- magnific popup CSS -->
    <link rel="stylesheet" href="css/magnific-popup.css">
    <!-- nice select CSS -->
    <link rel="stylesheet" href="css/nice-select.css">
    <!-- swiper CSS -->
    <link rel="stylesheet" href="css/slick.css">
    <!-- style CSS -->
    <link rel="stylesheet" href="css/style.css">
</head>

<body>
    <!--::header part start::-->
    <header class="main_menu">
        <div class="container">
            <div class="row align-items-center">
                <div class="col-lg-12">
                    <nav class="navbar navbar-expand-lg navbar-light">
                        <a class="navbar-brand" href="index.html"> <img src="img/omr_logo.png" alt="logo"> </a>
                        <button class="navbar-toggler" type="button" data-toggle="collapse"
                            data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                            aria-expanded="false" aria-label="Toggle navigation">
                            <span class="navbar-toggler-icon"></span>
                        </button>

                        <div class="collapse navbar-collapse main-menu-item justify-content-center"
                            id="navbarSupportedContent">
                            <ul class="navbar-nav align-items-center">
                                <li class="nav-item active">
                                    <a class="nav-link" href="index.html">Home</a>
                                </li>
                                <li class="nav-item">
                                    <a class="nav-link" href="problem.html">Problem</a>
                                </li>

                                <li class="nav-item dropdown">
                                    <a class="nav-link dropdown-toggle" href="blog.html" id="navbarDropdown"
                                        role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                                        Solution
                                    </a>
                                    <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                                      <a class="dropdown-item" href="assessing_risk.html">Assessing Risk</a>
                                      <a class="dropdown-item" href="tool_instructions.html">Tool Instructions</a>
                                      <a class="dropdown-item" href="technical.html">Technical Design</a>
                                    </div>
                                </li>
                            </ul>
                        </div>
                    </nav>
                </div>
            </div>
        </div>
    </header>
    <!-- Header part end-->

    <!-- breadcrumb start-->
    <section class="breadcrumb_part breadcrumb_bg">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <div class="breadcrumb_iner">
                        <div class="breadcrumb_iner_item">
                            <h2>Data and Modeling Approach</h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- breadcrumb start-->

    <!--Data -->
    <div class="whole-wrap">
      <div class="container box_1170">
        <div class="section-top-border">
          <h3 class="mb-30">The Data</h3>
          <div class="row">
            <div class="col-md-9">
              <p>Our tool was trained on data gathered by The National Survey on Drug Use and Health (NSDUH) during the years 2015-2017.
                NSDUH is a nationwide study providing information on tobacco, alcohol, drug-use, mental health, and other health-related issues
                in the general population of the United States. The study collects data via individual interviews with approximately 70,000 teens
                and adults each year and the results are used to inform various public health programs, policies, and tools; such as our OMR-Tool.
                For more information on the survey, please visit their website:</p>
                  <p><br><a class="btn_2 " href="https://nsduhweb.rti.org/" target = "_blank">NSDUH Website</a></p>

              <p><br>Although NSDUH data set dates back to 1971, using only recent data was a key design consideration, because the opioid crisis has grown and changed
                rapidly in recent years, and it is important for the model to make its predictions based on the current state of the opioid crisis. Additionally, the survey
                underwent a large number of changes starting in 2015, making earlier data difficult to integrate. For example, the Diagnostic and Statistical Manual of
                Mental Disorders (DSM-V) standards were incorporated into the survey in 2015, changing the diagnostic criteria for many of the included drug and mental health disorders.
                The raw data set contained approximately 170,000 rows, with each row corresponding to a different survey respondent (individual respondents are not allowed to take the
                survey more than once, in the same year or across multiple years). After removing participants who had not used prescription painkillers in the past year, the dataset contained
                approximately <b>53,000 respondents</b>.</p>
            </div>
            <div class="col-md-3">
              <img src="img/nsduh.png" alt="" class="img-fluid">
            </div>
          </div>
        </div>
      </div>
    </div>

  <!-- Feature Selection and pre-processing -->

  <section class="sample-text-area border-top">
    <div class="container box_1170">
      <h3 class="text-heading">Feature Selection and Pre-Processing</h3>
      <p class="sample-text">
The data set contains 2,631 features, each of which corresponds to either a question directly asked on the NSDUH survey,
or a recoded variable created from aggregating multiple questions. The outcome variable for this tool is “Opioid Misuse”,
which is defined as people who answered Yes to the question: <b>“Have you ever, even once, used any prescription pain reliever
in any way a doctor did not direct you to use it in the past 12 months?”</b>  This question is further defined to include using
prescription pain relievers <b>“without a prescription of your own”</b>, <b>“in greater amounts, more often, or longer than you were told
to take it”</b>, or <b>“in any other way a doctor did not direct you to use it."</b></p>

<p><br>Each survey respondent was categorized as having misused or not misused opioids in the past 12 months (from survey date).
For variables that had only categorical responses, we one-hot-encoded them before adding to our model. For variables with a
mix of categorical and continuous responses (e.g., “At what age did you first smoke cigarettes” with a response option for “never smoked”), the
continuous responses were first binned, and then one-hot-encoded along with the categorical responses.</p></br>
<h4> Feature Reduction</h4>
<p>One key challenge was to narrow the feature space down from 2,631 variables in the survey down to roughly 25 variables/questions,
which a patient could practically provide responses for on a form in a physician’s office.  Several strategies for choosing or excluding
features helped achieve this goal:</p>

<p><br><ol class="ordered-list">
  <li><span>Excluding features that were effectively asking the same question as the outcome variable.</span></li>
  <li><span>Excluding features that were very similar to other features in the same topic.</span></li>
  <li><span>Excluding features that only inquired about patient behaviors / activities in the past 30 days, as including
  these would have likely introduced significant look-ahead bias; since the outcome variable measured opioid misuse in the prior 12 months.</span></li>
  <li><span>Choosing feature categories based on what existing research has shown are the most likely contributors to opioid misuse:  patient age, tobacco
  use, other substance abuse (hard drugs), mental health, etc.</span></li>
  <li><span>Choosing features with high differences of misuse prevalence across their response options. Since N-size can be so impactful to this type of calculation, computing the weighted standard deviation of misuse
  prevalence across response options for each question provided an automated and robust way to quickly evaluate each feature.</span></li>
  <li><span>Excluding features from existing research that did not have a strong difference in their prevalence of opioid misuse across response options.</span></li>
  <li><span>Excluding features with perfect or near-perfect correlation with other features, as this multicollinearity could be harmful to both model performance and feature importance accuracy.</span></li>

</ol></p><br>

<h4>Potential Look-Ahead Bias</h4>
<p>Because our outcome variable and certain misuse features for other drugs were specifically about the previous 12 months, the team had to consider the possibility of look-ahead bias; that is,
  for individuals who misused both opioids and another substance in the previous 12 months, which of the misuses came first? Perhaps misusing opioids caused individuals to misuse other substances,
  which would mean misuse of the other drug would be an outcome rather than a feature; this would lead to over-predicting risk for individuals who have used these other substances. However, it could
  also be the other way around, where other substance misuse came first and therefore is a legitimate predictor of opioid misuse; or it could be the underlying predictor of a propensity to misuse opioids
  (e.g., a patient with an “addictive personality”), regardless of which one came first.  </p></br>

  <p>To deal with this potential bias, when possible, the team selected features that encompassed wider time-frames than the 12 month misuse window. This tactic was not always possible, but the direction
    of the look-ahead bias erred on the side of over-predicting risk, which was the more conservative and therefore more tolerable direction of bias. This was one of the main reasons why the team was okay
    allowing a few features with this potential bias.  If this project advances to the clinical trials stage, the team will prioritize eliminating look-ahead bias in a designed experiment.</p></br>

    <p>In conclusion, while there is potentially look-ahead bias, it is by no means definitive, nor is it a detractor of this project’s effectiveness. Given the few features impacted and conservative direction
       of the potential bias, along with the opportunity to eliminate it in future stages, the team accepted this risk but also felt it was important to acknowledge.</p></br>

    </div>
    </section>

 <!-- Model Selection and Eval -->
<section class="sample-text-area border-top">
    <div class="container box_1170">
      <h3 class="text-heading">Model Selection and Evaluation</h3>
      <p class="sample-text">
        <h4>Data Splitting</h4>
<p>We trained our model using 60% of the ~53,000 row dataset. After training the model, 25% of the data was used for calibration and validation, retaining the remaining 15% of the data to test model output.</p></br>
<h4> Model Selection</h4>
<p>The team tested numerous models of various complexity, starting with a simple logistic regression model to serve as a parsimonious baseline against which other models could be compared. 
    The next model attempted was an Extreme Gradient Boost (XGBoost or XGB) model, an implementation of gradient boosted decision trees known to work particularly well for this kind of relatively 
    small sized and dense data.  Finally, a relatively simple neural network was also tested to assess model performance using a completely different family of models.</p>

<p>All three of these initial models are “uncalibrated,” and among them, XGBoost performed considerably better than the other two (see next section, Model Evaluation, for performance criteria).  
    To further improve model performance, models were run through a calibrated classifier, which is specifically designed to improve performance of models’ predicted probabilities, and resulting in three additional calibrated versions of the models.</p>
    
<p>While ordinarily blackbox models such as XGBoost and neural networks have the undesirable tradeoff of sacrificing interpretability for performance, the use of Shapley values to communicate relative 
    feature importance allows users to determine which features increase or decrease a patient’s risk score, something the team deemed vital for tool usefulness and adoption.</p>

<h4>Model Evaluation</h4>
<p><br>Brier Loss Score was the primary evaluation metric used to determine model performance. Though perhaps not as well known as other binary classification methods, it has actually been used since 1950, and for this use case, 
    was the more appropriate method to use because it is specifically designed to evaluate probability predictions.  For example, in a group of 100 patients, all of whom have a predicted probability of approximately 20%, 
    Brier Loss Score will indicate the best performance for if 20 of those patients actually misused opioids and 80 did not.  Furthermore, it is important to remember, unlike is often the case of binary classification problems, 
    the goal of this project is to predict the probability of opioid misuse, not to predict the binary outcome of a patient misusing or not misusing opioids.  Therefore, traditional model evaluation techniques such as Precision, 
    Recall, F1 Score, and ROC AUC metrics were not helpful in evaluating desired probability outputs, as they focus on ratios of false positives and false negatives as opposed to measuring probability.<p>

<p><br>The team also generated calibration curves (a.k.a., reliability diagrams) from the test data for each model, which compare the relative frequency of what was observed to the predicted probability frequency. 
    While the best models are closest to the diagonal line representing a perfectly calibrated model (each probability predicted by the model is exactly correct), since no model is perfect, we preferred models 
    whose bias overestimated risk (the safe zone) rather than underestimated risk (the danger zone).  The diagram below depicts model performance on these calibration curves. </p></br> 
<img src="img/calibration_curves.png" alt="" class="img-fluid">
</br>

<p>In all the top performing models (those closest to the diagonal line), even at their worst performance, the predicted probabilities are only roughly seven percentage points away from the actual probabilities, 
a good practical validation of model performance.<\br></p>


<h4> Final Model </h4>
<p>The final model chosen for the OMR Tool is a calibrated XGBoost model. While the calibrated logistic regression model had a slightly lower Brier Loss Score, the team ultimately chose the calibrated XGBoost model because it incorporates feature interaction, 
    making the personalized feature importance output (Shapley values, see below) far more aligned with existing literature.  An example of the distribution of feature importance of the calibrated Logistic Regression model vs. the calibrated XGBoost model is shown here.</b></p>
<img src="img/model_compare.png" alt="" class="img-fluid"></br>

<h4> Shapley Values </h4>
<p>It is important for a doctor to know why each patient receives their particular risk score. While this task is a straightforward output for a simple uncalibrated logistic regression model, as model complexity increases, human understanding of the model typically decreases. 
    To add clarity for our users and to help doctors understand how risk works for each of their patients individually, the OMR Tool provides a visual output of each user’s top five features (i.e., the patient’s answers to each question) that contributed most to their total risk score, 
    displayed as a two-sided bar graph. A Shapley value tells us how much each feature contributes to the overall risk score. Mathematically, a Shapley value is the average marginal contribution of a feature value across all possible combinations of features from our model. 
    For one patient, their previous non-opioid substance abuse may have been the primary feature that brought their risk score up, but for another patient, even if they also have the same history of non-opioid substance abuse, it could be the simple fact of, for example, 
    being young and male that contributed most to their individual risk. Each patient is different, and the tool assesses each individual’s information holistically, rather than statically question by question as previous risk assessment tools have.</p>

<p><br>It is important to note, however, that this tool does not claim experimental causality between the features and opioid misuse; rather, it is claiming predictive association between the features and opioid misuse.  
    For example, it would be technically incorrect to say that being young and male causes a patient’s risk score to go up, but it is acceptable to say that being young and male is predictive of and/or contributes to a patient’s high risk score.  
    While achieving causal proof of the relationship between features and opioid misuse might be nice, the experiment required to achieve this claim is infeasible for many reasons.  Moreover, there is a long precedent in the medical world to use predictive association,
    as illustrated when a doctor asks if a patient has a family history of some condition. While family history can be predictive of a condition, it doesn’t mean that it’s been causally proven to drive that same condition, 
    yet doctors frequently use it as an indicator of patient risk for that condition.</p>

    </div>
    </section>
  
 <!-- Production Pipeline -->
<section class="sample-text-area border-top">
    <div class="container box_1170">
      <h3 class="text-heading">Production Pipeline</h3>
      <p class="sample-text">
<img src="img/production_pipeline.png" alt="" class="img-fluid">
</br>
<p><br>Our multi-step webform was developed using Django, a python web-framework. We pre-process the webform responses and feed them into our trained calibrated XGBoost model and evaluation scripts. 
    Our scripts output three important pieces of information: the likelihood of the patient to misuse opioids, the corresponding percentile for that likelihood within the general population, and personalized feature importance through use of Shapley values. 
    These three data objects are inputs for our report and data visualizations. Fusion Charts, a JavaScript charting library, are used for this task. Lastly, Bootstrap components render these charts in Django on a Patient Risk Report that a physician can reference when meeting with the patient.</p>



    </div>
    </section>

    <!-- footer part start-->

    <footer class="footer-area">
        <div class="footer section_padding">
            <div class="container text-center">
                  <h2>Contact Us</h2>
                  <h3 class="section-heading">
                    <a href="mailto:opioidmisuseriskteam@gmail.com" target="_top">OpioidMisuseRiskTeam@gmail.com</a>
                  </h3>
            </div>
        </div>


        <div class="copyright_part">
            <div class="container">
                <div class="row align-items-center">
                    <p class="footer-text m-0 col-lg-8 col-md-12"><!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | This template is made with <i class="ti-heart" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank">Colorlib</a>
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
</p>

                </div>
            </div>
        </div>
    </footer>

    <!-- footer part end-->

    <!-- jquery plugins here-->

    <script src="js/jquery-1.12.1.min.js"></script>
    <!-- popper js -->
    <script src="js/popper.min.js"></script>
    <!-- bootstrap js -->
    <script src="js/bootstrap.min.js"></script>
    <!-- owl carousel js -->
    <script src="js/owl.carousel.min.js"></script>
    <script src="js/jquery.nice-select.min.js"></script>
    <!-- contact js -->
    <script src="js/jquery.ajaxchimp.min.js"></script>
    <script src="js/jquery.form.js"></script>
    <script src="js/jquery.validate.min.js"></script>
    <script src="js/mail-script.js"></script>
    <script src="js/contact.js"></script>
    <!-- custom js -->
    <script src="js/custom.js"></script>
</body>

</html>
